import argparse

# CHANGE additional imports
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import csv
from torch.utils.data import DataLoader
from matplotlib import pyplot as plt
from torchvision import transforms
from oxford_pet import load_dataset
from models.unet import UNet
from models.resnet34_unet import ResNet34_UNet
from evaluate import evaluate
from PIL import Image

def set_seed(seed=42):
    """ è¨­å®šæ‰€æœ‰éš¨æ©Ÿæ•¸ç”Ÿæˆå™¨çš„ seed ä»¥ç¢ºä¿çµæœå¯é‡ç¾ """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # é‡å°å¤š GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False  # ä¿è­‰ determinismï¼Œä½†å¯èƒ½ç¨å¾®å½±éŸ¿æ•ˆèƒ½

# è¨­å®š Seedï¼Œç¢ºä¿ reproducibility
SEED = 48  # é€™æ˜¯ä½ çš„ Seedï¼Œå¯ä»¥æ ¹æ“šä½ çš„æœ€ä½³çµæœä¿®æ”¹
set_seed(SEED)

# è³‡æ–™è½‰æ›ï¼šå½±åƒèˆ‡é®ç½©éƒ½è½‰ç‚ºå›ºå®šå¤§å°çš„ Tensor
class SegmentationTransform:
    def __init__(self, size=(256, 256)):
        self.image_transform = transforms.Compose([
            transforms.Resize(size),
            transforms.ToTensor(),
            # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        self.mask_transform = transforms.Compose([
            transforms.Resize(size, interpolation=transforms.InterpolationMode.NEAREST),
            transforms.ToTensor(),
        ])

    def __call__(self, image, mask=None, trimap=None):
        if isinstance(mask, np.ndarray):
            mask = Image.fromarray(mask)

        image = self.image_transform(image)
        mask = self.mask_transform(mask)

        # mask æ˜¯ shape [1, H, W]ï¼Œè½‰æˆ numpy æ–¹ä¾¿è™•ç†
        mask_np = mask.numpy()[0]
        binary_mask = (mask_np == 1).astype("float32")  # åªä¿ç•™ label==1 çš„éƒ¨åˆ†
        mask = torch.from_numpy(binary_mask).unsqueeze(0)  # å†è½‰å› tensor
        
        return {
            "image": image,
            "mask": mask
        }
    
def train(args):
    # implement the training function here
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    transform = SegmentationTransform(size=(256, 256))

    # è¼‰å…¥ Dataset ä¸¦ç”¨ DataLoader åŒ…è£
    train_dataset = load_dataset(data_path=args.data_path, mode="train", transform=transform)
    valid_dataset = load_dataset(data_path=args.data_path, mode="valid", transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)
    valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)

    if args.model.lower() == 'unet':
        model = UNet(in_channels=3, out_channels=1)
    else:
        model = ResNet34_UNet(in_channels=3, out_channels=1)
    
    # ğŸš€ **ä½¿ç”¨å¤š GPU è¨“ç·´**
    if torch.cuda.device_count() > 1:
        print(f"> ä½¿ç”¨ {torch.cuda.device_count()} å¼µ GPU è¨“ç·´")
        model = torch.nn.DataParallel(model)  # è®“ PyTorch è‡ªå‹•åˆ†é…åˆ°å¤šå¼µ GPU

    model = model.to(device)

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)

    epoch = 0
    train_losses = []
    valid_losses = []
    dice_scores = []

    # Early Stopping ç›¸é—œè®Šæ•¸
    best_valid_loss = float('inf')  # åˆå§‹åŒ–ç‚ºæ­£ç„¡çª®å¤§
    patience_counter = 0  # è¨˜éŒ„é©—è­‰æå¤±æœªæ”¹å–„çš„æ¬¡æ•¸
    model_pth = ''

    # ğŸ”¹ åˆå§‹åŒ– CSV æª”æ¡ˆï¼Œå¯«å…¥æ¨™é¡Œ
    with open('training_log.csv', mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Epoch', 'Train Loss', 'Valid Loss', 'Dice Score'])
        
    for epoch in range(args.epochs):
        model.train()
        train_loss = 0.0
        for i, batch in enumerate(train_loader):
            images = batch['image'].to(device)
            masks = batch['mask'].to(device)
            print("> images shape:", images.shape)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)

            # æ¯ N ç­†è¼¸å‡ºä¸€æ¬¡è¨“ç·´é€²åº¦
            if (i + 1) % 10 == 0 or (i + 1) == len(train_loader):
                print(f"[Epoch {epoch+1}] Step {i+1}/{len(train_loader)}: Loss = {loss.item():.4f}")

        avg_train_loss = train_loss / len(train_loader.dataset)
        train_losses.append(avg_train_loss)

        # è©•ä¼°æ¨¡å‹
        avg_valid_loss, avg_dice_score = evaluate(net=model, data=valid_loader, device=device, criterion=criterion)

        '''
        model.eval()
        valid_loss = 0.0
        dice_score = 0.0
        with torch.no_grad():
            for batch in valid_loader:
                images = batch['image'].to(device)
                masks = batch['mask'].to(device)

                outputs = model(images)
                loss = criterion(outputs, masks)
                valid_loss += loss.item() * images.size(0)
                dice_score += dice_score(outputs, masks) * images.size(0)
        avg_valid_loss = valid_loss / len(valid_loader.dataset)
        avg_dice_score = dice_score / len(valid_loader.dataset)
        '''

        valid_losses.append(avg_valid_loss)
        dice_scores.append(avg_dice_score)
        
        print(f"Epoch {epoch+1}/{args.epochs} | Train Loss: {avg_train_loss:.4f} | "
              f"Valid Loss: {avg_valid_loss:.4f} | Dice Score: {avg_dice_score:.4f}")
        
        # ğŸ”¹ è¨˜éŒ„åˆ° CSV
        with open('training_log.csv', mode='a', newline='') as file:
            writer = csv.writer(file)
            writer.writerow([epoch, avg_train_loss, avg_valid_loss, avg_dice_score])

        # Early Stopping æª¢æŸ¥
        if avg_valid_loss < best_valid_loss:
            best_valid_loss = avg_valid_loss
            patience_counter = 0  # é‡ç½® patience counter
            if args.model.lower() == 'unet':
                model_pth = "unet_model_best.pth"
            else:
                model_pth = "resnet34_unet_model_best.pth"
            torch.save(model.state_dict(), model_pth)
            print(f"> é©—è­‰æå¤±æ”¹å–„ï¼Œå„²å­˜æ¨¡å‹ç‚º {model_pth} (Loss: {best_valid_loss:.4f})")

        else:
            patience_counter += 1
            print(f"> é©—è­‰æå¤±æœªæ”¹å–„ ({patience_counter}/{args.patience})")

        # åœæ­¢æ¢ä»¶
        if patience_counter >= args.patience:
            print("> Early stopping triggered. åœæ­¢è¨“ç·´ã€‚")
            break

    # å„²å­˜æ¨¡å‹
    if args.model.lower() == 'unet':
        model_pth = f"unet_model_{epoch+1}.pth"
    else:
        model_pth = f"resnet34_unet_model_{epoch+1}.pth"
    torch.save(model.state_dict(), model_pth)
    print(f"æ¨¡å‹å·²å„²å­˜ç‚º {model_pth}")

    # ğŸš€ **ç¢ºä¿ `epochs_range` é•·åº¦èˆ‡ `train_losses` ä¸€è‡´**
    epochs_range = np.arange(1, len(train_losses) + 1)

    # ğŸ“ˆ ç•«åœ–ä¸¦å„²å­˜
    plt.figure(figsize=(12, 5))

    # ğŸ“Œ Loss æ›²ç·š
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, train_losses, label="Train Loss")
    plt.plot(epochs_range, valid_losses, label="Valid Loss")
    plt.title("Loss over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()

    # ğŸ“Œ Dice Score æ›²ç·š
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, dice_scores, label="Dice Score", color="green")
    plt.title("Dice Score over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Dice Score")
    plt.legend()

    plt.tight_layout()
    graph_name = ''
    if args.model.lower() == 'unet':
        graph_name = "unet_training_results.png"
    else:
        graph_name = "resnet34_unet_training_results.png"
    plt.savefig(graph_name)
    print(f"è¨“ç·´çµæœæŠ˜ç·šåœ–å·²å„²å­˜ç‚º {graph_name}")

def get_args():
    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')
    parser.add_argument('--data_path', type=str, default='./dataset/oxford-iiit-pet', help='path of the input data') # CHANGE add the default path
    parser.add_argument('--epochs', '-e', type=int, default=100, help='number of epochs') # CHANGE default epochs 5 to 15
    parser.add_argument('--batch_size', '-b', type=int, default=32, help='batch size') # CHANGE default batch size 1 to 32
    parser.add_argument('--learning-rate', '-lr', type=float, default=1e-5, help='learning rate')
    parser.add_argument('--patience', type=int, default=10, help='early stopping patience')  # æ–°å¢ Early Stopping çš„åƒæ•¸
    parser.add_argument('--model', type=str, default='UNet', help='choose UNet or ResNet34_UNet')  # æ–°å¢ Early Stopping çš„åƒæ•¸
    return parser.parse_args()
 
if __name__ == "__main__":
    args = get_args()
    train(args)
